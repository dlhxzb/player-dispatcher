# 架构
服务分两层：
* dispatcher：分发服务器。管理所有地图服务器，后台线程监视各服务器负载，发起扩缩容请求。它有两个缓存：
  * 区域-服务器缓存，用于动态划分区域，负载均衡
  * 用户-服务器缓存，以此为依据将请求分发到地图服务器
* map-server：地图服务器，实现2个service
  * game_service：玩家请求
  * map_service：玩家导入导出等扩缩容相关请求

# 并发一致性
考虑到写请求不少，不太符合RwLock的应用场景。
为实现无锁，使用跳表（Skiplist）来记录服务器清单、玩家清单。  
另外跳表也无法保证先读后写是原子的，除非上锁，否则存在数据竞争。例如同时两次请求给玩家money+1，可能最后只+1。  
  
解决方案：考虑以玩家为单位将请求串行起来（ert crate）

# API流程
### login
* dispatcher根据坐标计算分到哪个地图服务器
* 地图服务器login
* dispatcher加入用户缓存

### aoe
* dispatcher根据范围坐标计算分到一台或几台地图服务器
* 各地图服务器处理aoe

### walking
* dispatcher从缓存中取出用户当前zone
* dispatcher计算移动目标zone，
  * 如果不是当前zone：
    * 调用当前地图服务器export_player到目标服务器，`为减轻dispatcher负担，这里不用它中转`
    * dispatcher更新用户-服务器缓存
    * 将walking请求发送给目标服务器
  * 如果是当前zone，则将walking发给原地图服务器

# 数据结构
地图区域划分按照四叉树结构，四个象限1234，递归向下划分，直至最大允许深度，e.g. MAX_ZONE_DEPTH=10。  
每次划分都会有四个象限，意味着每个父节点都有满4个子节点。  
叶子结点归地图服务器管理，而且一台服务器所管理的叶子结点必须是相同父节点
**对于最大深度第10层的区域，因为无法向下划分，此时一个叶子节点可以对应多台服务器**
也就是说对于所有叶子节点与服务器是N:1(N<4)，对于第10层叶子结点还可以是1:M
### 给定坐标对应区域查找
先把坐标换算成满树时叶子结点，按父节点逐层向上，直至找到有地图服务器的节点。
定位玩家坐标到zone：123，其中1固定代表根节点（全地图），全地图划分四份后的2象限，再划分后3象限。
如果最大深度为5层，算出来满树时玩家在区域12341，先查12341区域服务器是否创建，再查1234，然后123，12，1，最后0   
缺点是：最大有几层最多就要在跳表查几次，估计会影响性能

# 动态扩缩容流程
设服务器最大人数MAX（扩容），最低人数MIN（缩容），  

### 扩容
dispatcher监视到某一服务器玩家大于MAX，首先dispatcher启动一台服务器，
* 1. 根据过载服务器管理区域
  * 管理多个zone叶子结点的，或者管单个子节点但未达到最大深度的
    * 调用get_heaviest_zone_players，选出最大人数的zone以及其内的用户ID
  * 管理单节点且已达到最大允许深度
    * dispatcher调用get_n_player，获取一半用户ID
* 2. 更新区域-服务器缓存，此后该区域请求将转至新服务器
* 3. dispatcher调用export_player，根据用户ID逐个导入新服务器
* 4. dispatcher更新用户-服务器缓存

`3，4须与用户操作API串行，避免数据竞争。`  
这也是3.**逐个**用户导入的原因，一旦进行块传输那么就要给涉及到的所有用户上锁

### 缩容
dispatcher监视到某一服务器玩家小于MIN，尝试缩容。  
缩容是扩容的逆序，同样只能由同父的叶子结点之间合并。当4个叶子结点都归同一台服务器时，其父节点收缩为叶子结点。
  
缺点：缩容的时候只能同父叶子节点合并，如果合并不了，那么负载小的那个也无法和其它父节点下的合并，浪费性能